runMode = "BATCH"
dataSource = "HDFS"
singleRun=true
#HDFS File Batch configs
#For Batch transactions, details about the data is present here
#batchInfo - List of data sources for which RDD's will be created by the Application Manager
#batchInfo.name - Name of the batch datasource
#batchInfo.dataDirectory - HDFS directory where the datasource resides
#HDFS File Batch configs
#For Batch transactions, details about the data is present here
#batchInfo - List of data sources for which RDD's will be created by the Application Manager
#batchInfo.name - Name of the batch datasource
#batchInfo.dataDirectory - HDFS directory where the datasource resides
hdfsFileBatch = {
  batchTime = 10
  timerStartDelay = 1
  batchInfo = [
    {
      name = "testDataSplitFiles"
      dataDirectory = {
        local ="src/test/data/hdfs/filesplit/"
        }
      #split file into day. Processing file daywise.
      groupFile = {
        dateformat = "yyyy-MM-dd"
        startDateIndex = 9
        endDateIndex = 19
        offset=-8640
      }
      validation = {
        columns = ["starttime","duration","subscriberid","destinationip","httpmethod","httphost","httpuri","httpuseragent","httpcontenttype","httpresponsecode","httpcontentlength","upstream_data","downstream_data","deviceid","network_type","ran_type","region_location","base_station_location","do_not_track","privacy"]
        datatypes = ["Long","Integer","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String"]
        dateFormat = "yyyy-MM-dd HH:mm:ss"
        delimiter = ","
        minimumColumn = 20
        specialCharacterReplace = "\\000"
        rules = {
          subscriberid=[minLength(10),maxLength(20)]
          destinationip=[checkSpace]
        }
      }

    },
    {
      name = "secondSource"
      dataDirectory = {
        local ="src/test/data/hdfs/filesplit/"
      }
      #split file into day. Processing file daywise.
      groupFile = {
        dateformat = "yyyy-MM-dd"
        startDateIndex = 9
        endDateIndex = 19
        offset=-8640
      }
      validation = {
        columns = ["starttime","duration","subscriberid","destinationip","httpmethod","httphost","httpuri","httpuseragent","httpcontenttype","httpresponsecode","httpcontentlength","upstream_data","downstream_data","deviceid","network_type","ran_type","region_location","base_station_location","do_not_track","privacy"]
        datatypes = ["Long","Integer","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String","String"]
        dateFormat = "yyyy-MM-dd HH:mm:ss"
        delimiter = ","
        minimumColumn = 20
        specialCharacterReplace = "\\000"
        rules = {
          subscriberid=[minLength(10),maxLength(20)]
          destinationip=[checkSpace]
        }
      }

    },
    {
      name = "location"
      dataDirectory = {
        local = "src/test/data/location"
        saiph-devqa = "/midm/"
        prod = ""
      }
      fileFormat = "parquet"
      readFullDataset = "true"
    }
  ]
}
transactions = [{
  transactionName = "com.verizon.bda.trapezium.framework.apps.TestReadByDate"
  inputData = [{
    name = "testDataSplitFiles"
  },{
    name = "location"
  },{
    name = "secondSource"
  }]
  persistDataName = "testOutput2"
}]
