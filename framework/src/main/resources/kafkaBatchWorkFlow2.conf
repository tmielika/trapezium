runMode = "BATCH"
dataSource = "KAFKA"
singleRun=true
#Kafka Stream
#if your application is streaming data from HDFS, provide Topic/partitions details
#consumerGroup - Kafka consumer group name. If you have multiple instances of consumers running,
#they should have the same consumer group name
#batchTime - duration of each batch
#streamsInfo - It is a list of streams that you want Application Manager to create
#streamsInfo.name - name of Kafka stream. Workflow components will provide this name if they are interested in this stream
#streamsInfo.topicName - name of Kafka topic
kafkaTopicInfo = {
  consumerGroup = "KafkaBatchGroup"
  maxRatePerPartition: 970
  batchTime = "5"
  auto.offset.reset = "earliest"
  streamsInfo = [{
    name = "hdfsStream"
    topicName = "kafkaTopic2"
    validation = {
      columns = ["router","interface","oid","eventdate","elapsetime","inbytes","outbytes"]
      datatypes = ["String","String","Integer","Date","Long","Double","Double"]
      dateFormat = "yyyy-MM-dd HH:mm:ss"
      delimiter = "|"
      minimumColumn = 7
      rules = {}
    }
  }]
}

transactions = [{
  transactionName = "com.verizon.bda.trapezium.framework.apps.kafka.AppETL"
  inputStreams = [{
    name: "hdfsStream"
  }]
  persistStreamName = "hdfsFiltered"
  isPersist = "false"
}, {
  transactionName = "com.verizon.bda.trapezium.framework.apps.kafka.AlgorithmETL"
  inputStreams = [{
    name: "hdfsFiltered"
  }]
  persistStreamName = "hdfsDerived"
  isPersist = "false"
}, {
  transactionName = "com.verizon.bda.trapezium.framework.apps.kafka.AlgorithmEval"
  inputStreams = [{
    name: "hdfsDerived"
  }]
  persistStreamName = "hdfsAnomalies"
  isPersist = "true"
}]
